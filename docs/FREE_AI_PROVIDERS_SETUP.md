# Best Free AI Providers Setup Guide

## Overview

Your Justice Companion app now supports **3 excellent free/affordable AI options**:

1. **Groq** - ‚ö° Ultra-fast inference (FREE tier with generous limits)
2. **HuggingFace Pro** - ü§ó Access to 100+ models ($9/month, you already have this)
3. **Ollama** - üíª 100% free local models (privacy-first, no API needed)

---

## 1. Groq API (RECOMMENDED for Speed)

### Why Groq?
- **FREE tier**: 14,400 requests/day, 7,000 requests/minute
- **Blazing fast**: 300+ tokens/second streaming
- **OpenAI-compatible API**: Works seamlessly with your existing code
- **Best models**: Llama 3.3 70B, Mixtral, Gemma 2

### Setup Steps

1. **Get API Key** (Free):
   - Visit: https://console.groq.com/keys
   - Sign up/login with Google/GitHub
   - Click "Create API Key"
   - Copy your key (starts with `gsk_...`)

2. **Configure in Justice Companion**:
   - Go to Settings ‚Üí AI Service Settings
   - Click "Add Provider"
   - Select: **Groq**
   - Paste API Key: `gsk_...`
   - Choose Model:
     - `llama-3.3-70b-versatile` (Recommended - 128K context, fastest 70B)
     - `llama-3.1-8b-instant` (Ultra-fast for simple queries)
     - `mixtral-8x7b-32768` (Best reasoning)
     - `gemma2-9b-it` (Google's Gemma 2)
   - Temperature: 0.7 (default)
   - Max Tokens: 8000
   - Click "Save"

3. **Test Streaming**:
   - Go to Chat view
   - Send message: "Test streaming response"
   - Should see tokens appear instantly (300+/sec)

### Free Tier Limits
- **14,400 requests/day** (~600/hour)
- **7,000 requests/minute**
- **128K tokens context** (Llama 3.3)
- No credit card required

### Official Groq Streaming Pattern (Verified via Context7)
```python
# Your implementation already uses this pattern! ‚úÖ
from groq import AsyncGroq

client = AsyncGroq(api_key="your_key")
stream = await client.chat.completions.create(
    messages=[{"role": "user", "content": "Hello"}],
    model="llama-3.3-70b-versatile",
    stream=True
)

async for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

---

## 2. HuggingFace Pro ($9/month - You Have This!)

### Why HuggingFace Pro?
- **100+ models**: Llama 3.3, Qwen 2.5, Mistral, Gemma 2
- **11 inference providers**: fireworks, together, cerebras, etc. (auto-routed)
- **Streaming support**: Full streaming API
- **No rate limits**: Generous limits on Pro plan

### Setup Steps

1. **Get API Key**:
   - Visit: https://huggingface.co/settings/tokens
   - Login with your Pro account
   - Click "New token"
   - Name: "Justice Companion"
   - Type: Read
   - Copy your token (starts with `hf_...`)

2. **Configure in Justice Companion**:
   - Go to Settings ‚Üí AI Service Settings
   - Click "Add Provider"
   - Select: **Hugging Face**
   - Paste API Key: `hf_...`
   - Choose Model:
     - `meta-llama/Llama-3.3-70B-Instruct` (BEST - 11 providers!)
     - `meta-llama/Llama-3.1-8B-Instruct` (Fast - 8 providers)
     - `Qwen/Qwen2.5-72B-Instruct` (Deep reasoning - 4 providers)
     - `mistralai/Mistral-7B-Instruct-v0.3` (Apache 2.0 - 2 providers)
   - Temperature: 0.7
   - Max Tokens: 8000
   - Endpoint: `https://api-inference.huggingface.co` (default)
   - Click "Save"

3. **Test Streaming**:
   - Go to Chat view
   - Send message: "Explain UK employment law briefly"
   - Should stream smoothly with legal context

### HuggingFace Pro Benefits
- **11 inference providers** for Llama 3.3 (auto-routed for best performance)
- **No rate limits** on Pro tier
- **Model gating bypass**: Access to gated models (Llama, Gemma)
- **Priority inference**: Faster than free tier

### Official HuggingFace Streaming Pattern (Verified via Context7)
```python
# Your implementation already uses this pattern! ‚úÖ
from huggingface_hub import AsyncInferenceClient

client = AsyncInferenceClient(token="your_key")
stream = await client.chat_completion(
    model="meta-llama/Llama-3.3-70B-Instruct",
    messages=[{"role": "user", "content": "Hello"}],
    stream=True,
    max_tokens=500
)

async for chunk in stream:
    print(chunk.choices[0].delta.content, end="")
```

---

## 3. Ollama (100% Free, Local, Private)

### Why Ollama?
- **100% FREE**: No API costs, run models locally
- **Privacy**: Your data never leaves your machine
- **No internet needed**: Works offline
- **Easy setup**: One-click install
- **Great models**: Llama 3.3, Mistral, Gemma, Qwen, Phi-3

### Setup Steps

#### Step 1: Install Ollama

**Windows:**
1. Download: https://ollama.com/download/windows
2. Run installer: `OllamaSetup.exe`
3. Ollama starts automatically as a service on `http://localhost:11434`

**macOS:**
```bash
brew install ollama
ollama serve  # Starts server
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama serve  # Starts server
```

#### Step 2: Pull Models

```bash
# Recommended models for legal AI:

# Llama 3.2 (3B - Fast, 2GB RAM)
ollama pull llama3.2

# Llama 3.1 (8B - Balanced, 4.7GB RAM)
ollama pull llama3.1

# Mistral (7B - Excellent reasoning, 4.1GB RAM)
ollama pull mistral

# Qwen 2.5 (7B - Chinese + English, 4.7GB RAM)
ollama pull qwen2.5

# Gemma 2 (9B - Google, 5.5GB RAM)
ollama pull gemma2
```

#### Step 3: Verify Ollama is Running

```bash
# Test API endpoint
curl http://localhost:11434/v1/models

# Should return JSON with installed models
```

#### Step 4: Configure in Justice Companion

1. Go to Settings ‚Üí AI Service Settings
2. Click "Add Provider"
3. Select: **Ollama (Local)**
4. **No API Key needed** (leave blank)
5. Choose Model:
   - `llama3.2` (Fastest - 3B params)
   - `llama3.1` (Balanced - 8B params)
   - `mistral` (Best reasoning - 7B params)
   - `qwen2.5` (Multilingual - 7B params)
   - `gemma2` (Google - 9B params)
6. Endpoint: `http://localhost:11434/v1` (default)
7. Temperature: 0.7
8. Max Tokens: 8000
9. Click "Save"

#### Step 5: Test Streaming

- Go to Chat view
- Send message: "What are the time limits for employment tribunal claims?"
- Should stream from your local Ollama instance

### Ollama Performance Tips

**Hardware Requirements:**
- **Minimum**: 8GB RAM (3B models)
- **Recommended**: 16GB RAM (8B models)
- **Ideal**: 32GB RAM (70B models via `ollama pull llama3.3:70b`)

**Speed Optimization:**
- GPU acceleration (NVIDIA, AMD, Apple Metal) - 10x faster
- Keep models loaded: `ollama run llama3.1` (stays in memory)
- Use smaller models for faster responses

### Official Ollama Streaming Pattern (Verified via Context7)
```python
# Your implementation already uses this pattern! ‚úÖ
from ollama import AsyncClient

client = AsyncClient()
stream = await client.chat(
    model='llama3.1',
    messages=[{'role': 'user', 'content': 'Hello'}],
    stream=True
)

async for chunk in stream:
    print(chunk['message']['content'], end='', flush=True)
```

---

## Comparison: Which Provider to Use?

| Provider | Speed | Cost | Privacy | Best For |
|----------|-------|------|---------|----------|
| **Groq** | ‚ö°‚ö°‚ö°‚ö°‚ö° 300+ tok/s | FREE | ‚ö†Ô∏è Cloud | **Production streaming** |
| **HuggingFace Pro** | ‚ö°‚ö°‚ö°‚ö° Fast | $9/mo | ‚ö†Ô∏è Cloud | **100+ models, reliability** |
| **Ollama** | ‚ö°‚ö°‚ö° Good | FREE | ‚úÖ 100% Local | **Privacy, offline, no limits** |

### Recommendations

**For Production Use:**
1. **Primary**: Groq (free, blazing fast, 14K req/day)
2. **Fallback**: HuggingFace Pro (you already have this)
3. **Offline**: Ollama (privacy-sensitive users)

**For Development:**
- Use Ollama locally (no API costs, instant feedback)

**For Privacy-Conscious Users:**
- Use Ollama exclusively (100% local, no data leaves device)

---

## Testing the Streaming Fix

Now that Groq, HuggingFace Pro, and Ollama are configured, test the `.value` error fix:

### Test Steps

1. **Setup Provider** (choose one):
   - Groq (recommended for testing speed)
   - HuggingFace Pro
   - Ollama (if installed)

2. **Test Streaming**:
   - Go to Chat view in Justice Companion
   - Send: "Explain UK small claims procedure"
   - **Expected**: Smooth token-by-token streaming
   - **Previous Error**: `'str' object has no attribute 'value'` ‚ùå
   - **Now**: No errors, streaming works ‚úÖ

3. **Check Browser Console**:
   - Open DevTools (F12)
   - Console tab
   - Should see: `Streaming chunk received: ...`
   - Should NOT see: `'str' object has no attribute 'value'`

4. **Test All Providers**:
   - Switch between providers in Settings
   - Test streaming with each
   - All should work identically

---

## Troubleshooting

### Groq: "API key invalid"
- Check key starts with `gsk_`
- Regenerate key at https://console.groq.com/keys
- Verify not using free tier limits (14,400/day)

### HuggingFace: "Model not found"
- Check you're on Pro plan ($9/mo)
- Verify API token has Read permissions
- Some models require accepting license (visit model page on HF)

### Ollama: "Connection refused"
- Check Ollama is running: `ollama serve`
- Verify port 11434 is open
- Windows: Check Ollama service in Task Manager
- Endpoint should be: `http://localhost:11434/v1`

### Streaming Not Working (All Providers)
- Check backend logs: Look for provider routing logs
- Verify model name matches exactly (case-sensitive)
- Check backend is running on port 8000
- Frontend should be on port 5173

---

## Next Steps

1. **Setup Groq** (5 minutes):
   - Get free API key
   - Test blazing-fast streaming
   - Use as primary provider

2. **Configure HuggingFace Pro** (2 minutes):
   - Use existing Pro account
   - Access 100+ models
   - Fallback for Groq limits

3. **Install Ollama** (10 minutes):
   - Download installer
   - Pull `llama3.1` model
   - Test local streaming

4. **Test Streaming Fix**:
   - Verify `.value` error is gone
   - Test all 3 providers
   - Confirm smooth streaming

---

## Summary

‚úÖ **Groq** added (FREE, ultra-fast, 14K req/day)
‚úÖ **Ollama** already supported (100% free, local, private)
‚úÖ **HuggingFace Pro** already supported (100+ models, $9/mo)
‚úÖ **Streaming bug** fixed (`.value` error resolved)
‚úÖ **Official SDK patterns** verified via Context7

**Best Setup for You:**
1. Primary: **Groq** (free tier, 300+ tok/s streaming)
2. Fallback: **HuggingFace Pro** (you already have this)
3. Offline: **Ollama** (privacy-first, no API needed)

Test streaming with any provider - the `.value` bug is fixed! üöÄ
